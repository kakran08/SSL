{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlTCfTsJqob4PRLg45AvJB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aWlZclPwNu69"},"outputs":[],"source":["# PI : PI Network\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader,Subset\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["# -------------------------\n","# Step 1: Data Preparation\n","# -------------------------\n","\n","# Transform: Normalize MNIST images\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.1307,), (0.3081,))\n","])\n","\n","# Load full MNIST dataset\n","train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n","\n","# Create labeled subset (e.g., 100 examples evenly distributed across classes)\n","num_labels = 100\n","labels = np.array(train_dataset.targets)\n","labeled_idx = []\n","\n","for i in range(10):\n","    idx = np.where(labels == i)[0][:num_labels // 10]\n","    labeled_idx.extend(idx)\n","\n","unlabeled_idx = list(set(range(len(train_dataset))) - set(labeled_idx))\n","\n","labeled_dataset = Subset(train_dataset, labeled_idx)\n","unlabeled_dataset = Subset(train_dataset, unlabeled_idx)\n","\n","labeled_loader = DataLoader(labeled_dataset, batch_size=64, shuffle=True)\n","unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"],"metadata":{"id":"0U2DNvy0N8qk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Step 2: Define the Model\n","# -------------------------\n","\n","class PiCNN(nn.Module):\n","    \"\"\"Simple CNN for Î  Model\"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.dropout = nn.Dropout(0.5)\n","        self.fc1 = nn.Linear(9216, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))        # Conv layer 1\n","        x = F.relu(self.conv2(x))        # Conv layer 2\n","        x = F.max_pool2d(x, 2)           # Max pooling\n","        x = torch.flatten(x, 1)          # Flatten to [batch, features]\n","        x = self.dropout(x)              # Apply dropout\n","        x = F.relu(self.fc1(x))          # Fully connected\n","        x = self.fc2(x)                  # Output logits\n","        return x"],"metadata":{"id":"aSISyzlvN9Mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Step 3: Training Utilities\n","# -------------------------\n","\n","def add_noise(x, noise_std=0.15):\n","    \"\"\"Adds Gaussian noise to an input tensor (augmentation)\"\"\"\n","    return x + torch.randn_like(x) * noise_std\n","\n","def train_pi_model(model, optimizer, labeled_loader, unlabeled_loader, alpha):\n","    model.train()\n","    for (x_l, y_l), (x_u, _) in zip(labeled_loader, unlabeled_loader):\n","        x_l, y_l = x_l.to(device), y_l.to(device)\n","        x_u = x_u.to(device)\n","\n","        # Supervised loss on labeled data (with dropout)\n","        logits_l = model(x_l)\n","        loss_sup = F.cross_entropy(logits_l, y_l)\n","\n","        # Consistency loss on unlabeled data\n","        # Pass same input twice with different dropout masks and augmentations\n","        x_u1 = add_noise(x_u)\n","        x_u2 = add_noise(x_u)\n","\n","        logits_u1 = model(x_u1)\n","        logits_u2 = model(x_u2)\n","\n","        probs_u1 = F.softmax(logits_u1, dim=1)\n","        probs_u2 = F.softmax(logits_u2, dim=1)\n","\n","        loss_unsup = F.mse_loss(probs_u1, probs_u2)\n","\n","        # Total loss\n","        loss = loss_sup + alpha * loss_unsup\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","def evaluate(model, loader):\n","    model.eval()\n","    correct = 0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            output = model(x)\n","            pred = output.argmax(dim=1)\n","            correct += (pred == y).sum().item()\n","    return correct / len(loader.dataset)"],"metadata":{"id":"pb9OPKfKOHa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------\n","# Step 4: Train the Model\n","# -------------------------\n","\n","model = PiCNN().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","# incase of two different models\n","# use\n","# optimizer = optim.Adam(list(modelA.parameters()) + list(modelB.parameters()), lr=1e-3)\n","\n","epochs = 20\n","alpha = 20.0  # weight for consistency loss\n","\n","for epoch in range(1, epochs + 1):\n","    train_pi_model(model, optimizer, labeled_loader, unlabeled_loader, alpha)\n","    test_acc = evaluate(model, test_loader)\n","    print(f\"Epoch {epoch:02d} | Test Accuracy: {test_acc:.4f}\")"],"metadata":{"id":"HY24OjS_OMms"},"execution_count":null,"outputs":[]}]}